{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Grammar Problem Generation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"nrHzdhA-Q1at"},"source":["# 변수 설정\n","VBP = \"VBP\"\n","VBN = \"VBN\"\n","VBG = \"VBG\"\n","VBD = \"VBD\"\n","VBZ = \"VBZ\"\n","RBR = \"RBR\"\n","RB = \"RB\"\n","RBS = \"RBS\"\n","NN = \"NN\"\n","NNS = \"NNS\"\n","JJ = \"JJ\"\n","JJS = \"JJS\"\n","JJR = \"JJR\"\n","TO_VERB = \"TO_VERB\"\n","DT = 'DT'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"txLaOMJIQZdX","executionInfo":{"status":"ok","timestamp":1622491208161,"user_tz":-540,"elapsed":297,"user":{"displayName":"­강수빈(학부/공과대학 컴퓨터과학)","photoUrl":"","userId":"07352247864639375532"}},"outputId":"bc733b22-60ad-4b26-ad5c-ccbc2162c719"},"source":["import pandas as pd\n","\n","problem_df = pd.read_csv('어법 문제.csv', index_col=0)\n","problem_df = problem_df.dropna()\n","problem_df['정답 text']\n","\n","s = problem_df.iloc[3,6]\n","s  "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'One way for a salesperson to gather knowledge about competitors is to study available literature and bulletins published on competitive products. These bulletins can usually be obtained at trade shows. Additional facts can be gathered by reading trade journals and competitors’ advertisements. It may be helpful for the salesperson to cut out and save each ad run by the competitors, because these ads reveal what products they are pushing. These ads will also reveal what features and benefits they are promoting on their products. Having this information enables a salesperson to make counter pitches. A salesperson can also compare competitive ads and bulletins with his company’s material and perhaps assist the advertising department in improving the company’s brochures and advertising.'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ4-Lpiz98d0","executionInfo":{"status":"ok","timestamp":1622491224536,"user_tz":-540,"elapsed":563,"user":{"displayName":"­강수빈(학부/공과대학 컴퓨터과학)","photoUrl":"","userId":"07352247864639375532"}},"outputId":"53600d19-8186-4802-93e1-149694bcd743"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"04F9HGgn-HE6"},"source":["어법 문제 Generate 함수"]},{"cell_type":"code","metadata":{"id":"AZkvSd2h9Kg6"},"source":["from nltk.tokenize import sent_tokenize\n","import random\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","\n","\n","\n","def generate_problem(paragraph, question_type) :\n","  nlp = spacy.load('en_core_web_sm')\n","\n","  sentences = [(sent, idx) for idx, sent in enumerate(sent_tokenize(paragraph))] # 문장 전체 list\n","\n","  if question_type == 1:  # 1,2,3,4,5 중에 틀린거 하나\n","    trans_num = 5     \n","    answer_type = ['①', '②', '③', '④', '⑤']   \n","  else:  # ABC\n","    trans_num = 3\n","    answer_type = ['(A)', '(B)', '(C)']   \n","\n","\n","  # 바꿀 문장 개수(문제 형태 유형)에 따라 바꿀 대상 선정 \n","  trans_before = random.sample(sentences, trans_num) #바꿀 대상 선정\n","  trans_after = []\n","  new_sentences = [sent[0] for sent in sentences]\n","  \n","  # pos mapping\n","  # TO_VERB : 분사 앞에 be동사가 있을 경우 be동사에 맞게 수일치, 시제 맞춰서 변환시켜야 함. \n","  pos_trans_dict = {'VBP': ['VBN', 'VBZ', 'VBD'], 'VBG' : ['TO_VERB', 'VBN'],  \n","                    'VBN':['TO_VERB', 'VBG'], 'VBZ':['VBN', 'VBZ', 'VBD'], \n","                    'NN':['NNS'], 'NNS':['NN'], \n","                    'DT':[{\"this\":\"these\", \"these\":'this', \"that\":'those', 'those':'that'}]}\n","                        \n","                        \n","                        \n","                         #'RB':['JJ'], 'RBR':['JJR'], 'RBS':['JJS'], \n","  determiner_transform_dict = {\n","      \"this\": [\"these\", \"that\", \"those\"],\n","      \"these\": [\"this\", \"those\",\"that\"],\n","      \"those\":[\"that\",\"this\",\"these\"],\n","      \"another\": [\"one\"],\n","      \"some\":[\"another\"],\n","      }\n","  determiner_transform_dict_small = {\"this\":[\"these\"], \"these\":['this'], \"that\":['those'], 'those':['that']}\n","  be_verb = ['be', 'was', 'were', 'being', 'been', 'am', 'are', 'is']\n","  answers = []\n","  answer_return = {}\n","  #  {\"A\": {\"answer\": \"is\", \"wrong_answer\": \"was\"},...}\n","  # convert sentence\n","  \n","  print(trans_before)\n","  trans_before = sorted(trans_before, key=lambda x: x[1])\n","  print(trans_before)\n","\n","  nlp = spacy.load('en_core_web_sm')\n","  for trans_idx, sent_obj in enumerate(trans_before) :\n","    # 우리가 변환 가능한 pos를 가지고 있는 단어들만 모으기\n","    sent_idx = sent_obj[1]\n","    sent = sent_obj[0]\n","    tokens = nlp(sent)\n","    avail_tokens = []\n","    new_tokens = [token.text for token in tokens]\n","    for idx,token in enumerate(tokens):\n","      # print(\"sentence\" , idx)\n","      tag = token.tag_\n","      # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n","      \n","\n","      if(tag in pos_trans_dict.keys()):\n","        # 지시대명사의 경우 따로 처리 \n","        if tag=='DT' and not token.text in pos_trans_dict['DT'][0].keys() :\n","          print(token.text)\n","          pass\n","        else :\n","          if(tokens[idx-1]):\n","            prev_tokens = tokens[idx-1]\n","          else:\n","            prev_tokens = None\n","          avail_tokens.append([token, idx, prev_tokens])\n","    \n","    while True:\n","      print(avail_tokens)\n","      src_token_obj = random.choice(avail_tokens)\n","      src_token = src_token_obj[0]\n","      src_tag = src_token.tag_\n","      print(src_token_obj)\n","\n","      # DT의 경우 \n","      if src_tag == \"DT\":\n","        target_pos = pos_trans_dict[\"DT\"][0][src_token.text]\n","        answer = chg_pos(src_token_obj, target_pos)\n","        if(answer):\n","          pos_trans_dict.pop('DT')\n","          answers.append((src_token_obj, answer))\n","          break\n","        else:\n","          continue\n","\n","      else :\n","        target_pos = random.choice(pos_trans_dict[src_tag])\n","        answer = chg_pos(src_token_obj, target_pos)\n","        if(answer):\n","          print(src_tag, target_pos)\n","          pos_trans_dict[src_tag].remove(target_pos)\n","          if(len(pos_trans_dict[src_tag]) == 0):\n","            pos_trans_dict.pop(src_tag)\n","          answers.append((src_token_obj, answer))\n","          break \n","        else:\n","          continue\n","\n","\n","      #elif src_tag == \"VBG\" or src_tag == \"VBN\"\n","    answer = answers[trans_idx]\n","    print(\"answer print\")\n","    print(answer[0][0].text)\n","    print(answer[1])\n","    print(type(tokens))\n","    new_tokens[answer[0][1]] = answer_type[trans_idx] + \"[\" + answer[0][0].text + '/' + answer[1] + \"]\"\n","    new_sen = TreebankWordDetokenizer().detokenize(new_tokens)\n","    new_sentences[sent_idx] = new_sen\n","    answer_dict = {'answer': answer[0][0].text, 'wrong_answer':answer[1] }\n","    answer_return[answer_type[trans_idx]] = answer_dict\n","    \n","   \n","   \n","  return ' '.join(new_sentences), answer_return\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XwS7P7Vhj9q"},"source":["pos 태그 변환 함수"]},{"cell_type":"code","metadata":{"id":"QsANywylOmyd"},"source":["pos_trans_dict = {'VBP': ['VBN', 'VBZ', 'VBD'], 'VBG' : ['TO_VERB', 'VBN'],  'VBN':['TO_VERB', 'VBG'], 'VBZ':['VBN', 'VBZ', 'VBD'], \n","                         'NN':['NNS'], 'NNS':['NN'], 'DT':[{\"this\":\"these\", \"these\":'this', \"that\":'those', 'those':'that'}]}\n","pos_det_dict = {\"this\":\"these\", \"these\":'this', \"that\":'those', 'those':'that'} #'RB':['JJ'], 'RBR':['JJR'], 'RBS':['JJS'],\n","be_verb = ['be', 'was', 'were', 'being', 'been', 'am', 'are', 'is']\n","import random\n","import spacy\n","import pyinflect\n","def chg_pos(src_token_obj, target_pos) : \n","  nlp = spacy.load('en_core_web_sm')\n","  src_token = src_token_obj[0]\n","  src_text = src_token.text\n","  src_tag = src_token.tag_\n","  print(src_tag, target_pos)\n","  target_token = ''\n","  print(\"chg_pos\")\n","  if(src_tag == DT) :\n","    target_token = pos_det_dict[src_text]\n","  elif(target_pos == TO_VERB) :\n","    prev_token_pos = src_token_obj[2].pos_\n","    target_token = src_token._.inflect(prev_token_pos)\n","  elif(src_tag == VBP and target_pos == VBN):\n","    if(src_text in be_verb):  \n","      target_token = None\n","    else :\n","      target_token = 'are' + src_token._.inflect(target_pos)\n","  elif(src_tag in [RB, RBR, RBS]):\n","    try:\n","      src_lemma = src_token.lemma_\n","      src_all = getAllInflections(src_lemma, pos_type='A')\n","      target_token = src_all[target_pos][0]\n","    except:\n","      traget_token = None\n","  else:\n","    target_token = src_token._.inflect(target_pos)\n","\n","  if(src_text == target_token):\n","    target_token = None\n","\n","  return target_token\n","    \n","  \n","  \n","  \n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PeVbdfoh2Qg"},"source":["generate_problem(s,2)"],"execution_count":null,"outputs":[]}]}